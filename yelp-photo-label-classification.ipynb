{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Создаем датасет из наших фото","metadata":{"id":"HhwqMDdFqojb"}},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf","metadata":{"id":"yJg1FSMZqojr","execution":{"iopub.status.busy":"2021-05-23T08:24:15.311029Z","iopub.execute_input":"2021-05-23T08:24:15.311397Z","iopub.status.idle":"2021-05-23T08:24:20.133783Z","shell.execute_reply.started":"2021-05-23T08:24:15.311301Z","shell.execute_reply":"2021-05-23T08:24:20.132918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#функция для чтения json\ndef load_rows(filepath, nrows = None):\n    with open(filepath, encoding='utf-8') as json_file:\n        count = 0\n        objs = []\n        line = json_file.readline()\n        while (nrows is None or count < nrows) and line:\n            count += 1\n            obj = json.loads(line)\n            objs.append(obj)\n            line = json_file.readline()\n        return pd.DataFrame(objs)","metadata":{"id":"tvZd-Vi3qojv","execution":{"iopub.status.busy":"2021-05-23T08:24:26.402866Z","iopub.execute_input":"2021-05-23T08:24:26.403192Z","iopub.status.idle":"2021-05-23T08:24:26.409074Z","shell.execute_reply.started":"2021-05-23T08:24:26.403164Z","shell.execute_reply":"2021-05-23T08:24:26.408044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#download photos.json as dataframe\nphotos = load_rows('../input/yelp-photos/photos.json')\nphotos.describe()","metadata":{"id":"iV7GWeZ6qojx","outputId":"1acd1e2b-eaff-4909-efd7-728cfce15b04","execution":{"iopub.status.busy":"2021-05-23T08:24:30.095833Z","iopub.execute_input":"2021-05-23T08:24:30.096188Z","iopub.status.idle":"2021-05-23T08:24:31.936669Z","shell.execute_reply.started":"2021-05-23T08:24:30.096158Z","shell.execute_reply":"2021-05-23T08:24:31.935934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Add photo path\nphotos['photo_path'] = photos['photo_id'] + '.jpg'\nphotos.head()","metadata":{"id":"cV1SDQWJqojy","outputId":"d041b0c7-3922-412d-c934-9012d9ccfa69","execution":{"iopub.status.busy":"2021-05-23T08:24:35.098198Z","iopub.execute_input":"2021-05-23T08:24:35.098557Z","iopub.status.idle":"2021-05-23T08:24:35.144858Z","shell.execute_reply.started":"2021-05-23T08:24:35.098526Z","shell.execute_reply":"2021-05-23T08:24:35.143956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shuffling and getting rid of menu label\nphotos = pd.concat([photos[photos['label']=='food'][:12000],photos[photos['label']=='inside'][:12000],\n                        photos[photos['label']=='outside'][:12000], photos[photos['label']=='drink'][:12000]])\nphotos = photos.sample(frac=1).reset_index(drop=True)\n","metadata":{"id":"sNbn0-BTwrl7","execution":{"iopub.status.busy":"2021-05-23T08:24:39.101843Z","iopub.execute_input":"2021-05-23T08:24:39.102221Z","iopub.status.idle":"2021-05-23T08:24:39.364452Z","shell.execute_reply.started":"2021-05-23T08:24:39.102189Z","shell.execute_reply":"2021-05-23T08:24:39.363534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"photos['label'].value_counts()","metadata":{"id":"2jWls7uLqojz","outputId":"8a2eed80-608d-408f-bad5-9267a07a0e84","execution":{"iopub.status.busy":"2021-05-21T15:43:32.54954Z","iopub.execute_input":"2021-05-21T15:43:32.549904Z","iopub.status.idle":"2021-05-21T15:43:32.570328Z","shell.execute_reply.started":"2021-05-21T15:43:32.549872Z","shell.execute_reply":"2021-05-21T15:43:32.569186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating dataset with business","metadata":{}},{"cell_type":"code","source":"#download json as dataframe\nbusiness = load_rows('../input/yelp-photos/yelp_academic_dataset_business.json')\nbusiness.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:02.151683Z","iopub.execute_input":"2021-05-22T16:50:02.152044Z","iopub.status.idle":"2021-05-22T16:50:07.959383Z","shell.execute_reply.started":"2021-05-22T16:50:02.152011Z","shell.execute_reply":"2021-05-22T16:50:07.958585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Убираем лишний столбцы, соединяем в один dataframe\ndf = pd.merge(photos, business, how ='inner', on ='business_id')\ndf = df.drop (columns = ['address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'hours'])\ndf","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:12.523458Z","iopub.execute_input":"2021-05-22T16:50:12.523814Z","iopub.status.idle":"2021-05-22T16:50:12.965481Z","shell.execute_reply.started":"2021-05-22T16:50:12.523783Z","shell.execute_reply":"2021-05-22T16:50:12.964469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate from dataframe","metadata":{}},{"cell_type":"code","source":"datagen=ImageDataGenerator(rescale=1./255.)\ntest_datagen=ImageDataGenerator(rescale=1./255.)\nbatch_size = 50\nnTrain=48000 \nphotos_dir = '../input/photos/photos'\n\n\ntrain_generator=datagen.flow_from_dataframe(df[:int(nTrain*0.7)],\n                                            photos_dir, x_col = 'photo_path', y_col = 'label',\n                                            target_size=(224, 224),\n                                            batch_size=batch_size,\n                                            class_mode='categorical', subset = 'training',\n                                            shuffle=True)\nvalid_generator=test_datagen.flow_from_dataframe(df[int(nTrain*0.7):int(nTrain*0.9)],\n                                                 photos_dir, x_col = 'photo_path', y_col = 'label',\n                                                 target_size=(224, 224),\n                                                 batch_size=batch_size,\n                                                 class_mode='categorical',\n                                                 shuffle=True)\ntest_generator=test_datagen.flow_from_dataframe(df[int(nTrain*0.9):],\n                                                photos_dir, x_col = 'photo_path', y_col = 'label',\n                                                target_size=(224, 224),\n                                                batch_size=batch_size,\n                                                class_mode='categorical',\n                                                shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T08:24:48.419159Z","iopub.execute_input":"2021-05-23T08:24:48.419505Z","iopub.status.idle":"2021-05-23T08:24:48.717983Z","shell.execute_reply.started":"2021-05-23T08:24:48.419474Z","shell.execute_reply":"2021-05-23T08:24:48.716035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_datagen=ImageDataGenerator(rescale=1./255.)\nbatch_size = 50\nnTrain=46000 \nphotos_dir = '../input/photos/photos'\ntest_generator=test_datagen.flow_from_dataframe(photos[int(nTrain*0.9):],\n                                                photos_dir, x_col = 'photo_path', y_col = 'label',\n                                                target_size=(224, 224),\n                                                batch_size=batch_size,\n                                                class_mode='categorical',\n                                                shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:46:55.581998Z","iopub.execute_input":"2021-05-20T13:46:55.582341Z","iopub.status.idle":"2021-05-20T13:46:56.387125Z","shell.execute_reply.started":"2021-05-20T13:46:55.582291Z","shell.execute_reply":"2021-05-20T13:46:56.386343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Flow from directory generator\n","metadata":{"id":"oJrv76waL_I2"}},{"cell_type":"code","source":"# generate batches of train images and labels\nnTrain=48000\nphotos_dir = '../input/photos/photos'\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n# load the normalized images\ndatagen = ImageDataGenerator(rescale=1./255, validation_split = 0.2)\n# define the batch size\nbatch_size = 50\n\n# the defined shape is equal to the network output tensor shape\ntrain_features = np.zeros(shape=(nTrain, 7, 7, 512))\ntrain_labels = np.zeros(shape=(nTrain,4))\n# generate batches of train images and labels\ntrain_generator = datagen.flow_from_dataframe(photos[:nTrain],\n    photos_dir, x_col = 'photo_path', y_col = 'label',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='categorical', subset = 'training',\n    shuffle=True)","metadata":{"id":"Zerv2UXAqoj2","outputId":"53fade7d-c9f8-4030-d02f-bbb23915f7d0","execution":{"iopub.status.busy":"2021-05-23T08:25:14.52813Z","iopub.execute_input":"2021-05-23T08:25:14.528514Z","iopub.status.idle":"2021-05-23T08:26:44.062341Z","shell.execute_reply.started":"2021-05-23T08:25:14.528475Z","shell.execute_reply":"2021-05-23T08:26:44.061274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate batches of validation images and labels\nvalidation_generator = datagen.flow_from_dataframe(photos[:nTrain],\n    photos_dir, x_col = 'photo_path', y_col = 'label',\n    target_size=(224, 224),\n    batch_size=batch_size,color_mode=\"rgb\",\n    class_mode='categorical', subset = 'validation',\n    shuffle=True)","metadata":{"id":"sV_om7Dcqoj2","outputId":"ca3c3924-073d-4128-e02a-5e0d469135af","execution":{"iopub.status.busy":"2021-05-23T08:26:44.063993Z","iopub.execute_input":"2021-05-23T08:26:44.06458Z","iopub.status.idle":"2021-05-23T08:27:01.558411Z","shell.execute_reply.started":"2021-05-23T08:26:44.064536Z","shell.execute_reply":"2021-05-23T08:27:01.55747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the necessary packages\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import ResNet50\nfrom sklearn.metrics import classification_report\n#from imutils import paths\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"id":"sAHwgX0Kqoj3","execution":{"iopub.status.busy":"2021-05-23T08:27:08.827683Z","iopub.execute_input":"2021-05-23T08:27:08.827997Z","iopub.status.idle":"2021-05-23T08:27:09.387052Z","shell.execute_reply.started":"2021-05-23T08:27:08.827968Z","shell.execute_reply":"2021-05-23T08:27:09.386094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature extarction","metadata":{"id":"CzJp5HdUs72g"}},{"cell_type":"code","source":"# Get feature vector of an image by given model and img_path\ndef getFeatureVector(model, img_path):\n  img = cv2.imread(img_path)\n  img = cv2.resize(img, (224, 224))\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  feature_vector = model.predict(img.reshape(1, 224, 224, 3))\n  return feature_vector","metadata":{"id":"pukv52_XtBV-","execution":{"iopub.status.busy":"2021-05-21T07:56:57.44576Z","iopub.execute_input":"2021-05-21T07:56:57.446217Z","iopub.status.idle":"2021-05-21T07:56:57.452771Z","shell.execute_reply.started":"2021-05-21T07:56:57.446175Z","shell.execute_reply":"2021-05-21T07:56:57.451458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for get dataframe which contains the output features of given model\ndef getFeatureDataFrame(model):\n  df = pd.DataFrame(columns=['file', 'features'])\n  train_files = train_generator.filepaths\n  valid_files = validation_generator.filepaths\n  files = train_files + valid_files\n\n  df['file'] = files\n  df['features'] = df.apply(lambda row: getFeatureVector(model, row['file']), axis=1) \n\n  print(\"All files added.\")\n  return df","metadata":{"id":"-C0cjulPuvIC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get VGG-16 Model\ndef getVGG16Model(lastFourTrainable=False):\n  vgg_model = VGG16(weights='imagenet', input_shape=input_shape, include_top=True)\n\n  # Make all layers untrainable\n  for layer in vgg_model.layers[:]:\n      layer.trainable = False\n\n  # Add fully connected layer which have 1024 neuron to VGG-16 model\n  output = vgg_model.get_layer('fc2').output\n  output = Flatten(name='new_flatten')(output)\n  output = Dense(units=1024, activation='relu', name='new_fc')(output)\n  output = Dense(units=4, activation='softmax')(output)\n  vgg_model = Model(vgg_model.input, output)\n\n  # Make last 4 layers trainable if lastFourTrainable == True\n  if lastFourTrainable == True:\n    vgg_model.get_layer('block5_conv3').trainable = True\n    vgg_model.get_layer('fc1').trainable = True\n    vgg_model.get_layer('fc2').trainable = True\n    vgg_model.get_layer('new_fc').trainable = True\n\n  # Compile VGG-16 model\n  vgg_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n  vgg_model.summary()\n\n  return vgg_model","metadata":{"id":"RV1zdATKwpRV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nfrom keras.utils import np_utils\nfrom keras.models import Model, Sequential, load_model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Reshape, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.resnet50 import ResNet50\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport itertools\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"id":"mmGv87q7w0Yb","execution":{"iopub.status.busy":"2021-05-21T11:35:45.357147Z","iopub.execute_input":"2021-05-21T11:35:45.357478Z","iopub.status.idle":"2021-05-21T11:35:46.083116Z","shell.execute_reply.started":"2021-05-21T11:35:45.357429Z","shell.execute_reply":"2021-05-21T11:35:46.082316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constants\nimg_rows = 224\nimg_cols = 224\ninput_shape = (img_rows,img_cols,3)\nepochs = 10\nbatch_size = 50\nnum_of_classes = 4","metadata":{"id":"2OvNaBQcw-Qx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get feature extractor model from last layer of vgg_model_a\nvgg_model_a = getVGG16Model(lastFourTrainable=False)\n#vgg_model_a.load_weights('/content/drive/MyDrive/Colab Notebooks/cinic-10/model_vgg_nontrainable.h5')\nfeature_model_vgg_a = Model(inputs=vgg_model_a.input, outputs=vgg_model_a.get_layer('new_fc').output)\n\ndf = getFeatureDataFrame(feature_model_vgg_a)","metadata":{"id":"woghTryLux3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_pickle(\"features_vgg_a.pickle\")","metadata":{"id":"K9QktHr5u44A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for row in df.itertuples():\n  df['photo_id'][row.Index] = str(df['photo_id'][row.Index]).replace(\"/content/drive/MyDrive/для диплома/photos/\",\"\")\n  df['photo_id'][row.Index] = str(df['photo_id'][row.Index]).replace(\".jpg\",\"\")\ndf","metadata":{"id":"vqBLoYDrBJKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.merge(photos, on= 'photo_id')\ndf","metadata":{"id":"9AFqUsmKEGTT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df['features']\ny = df['label']","metadata":{"id":"vUlNxFgKEgGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"id":"AvSLXpO0E8wB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.convert_to_tensor(X_train)","metadata":{"id":"vmNUk8k2GrpT","outputId":"6d82968c-724b-4659-9470-2530443c5a10"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Second try on feature extraction","metadata":{"id":"EKB09Fx1HHx9"}},{"cell_type":"code","source":"from tensorflow.keras.applications import vgg16\n\nvgg_conv = vgg16.VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(224, 224, 3))","metadata":{"id":"dmGQ_AA1HEEw","execution":{"iopub.status.busy":"2021-05-21T12:12:11.701896Z","iopub.execute_input":"2021-05-21T12:12:11.702228Z","iopub.status.idle":"2021-05-21T12:12:12.793173Z","shell.execute_reply.started":"2021-05-21T12:12:11.702197Z","shell.execute_reply":"2021-05-21T12:12:12.792092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nTrain = 24000\nbatch_size = 50\n# the defined shape is equal to the network output tensor shape\ntrain_features = np.zeros(shape=(nTrain, 7, 7, 512))\ntrain_labels = np.zeros(shape=(nTrain,4))","metadata":{"id":"4fqmkfoSHUNW","execution":{"iopub.status.busy":"2021-05-21T08:13:43.620936Z","iopub.execute_input":"2021-05-21T08:13:43.622345Z","iopub.status.idle":"2021-05-21T08:13:43.638025Z","shell.execute_reply.started":"2021-05-21T08:13:43.622297Z","shell.execute_reply":"2021-05-21T08:13:43.636949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate through the batches of train images and labels\nfor i, (inputs_batch, labels_batch) in enumerate(train_generator):\n    if i % 50 == 0:\n        print(i)\n    if i * batch_size >= nTrain:\n        break   \n    # pass the images through the network\n    features_batch = vgg_conv.predict(inputs_batch)\n    train_features[i * batch_size : (i + 1) * batch_size] = features_batch\n    train_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n# reshape train_features into vector       \ntrain_features_vec = np.reshape(train_features, (nTrain, 7 * 7 * 512))\nprint(\"Train features: {}\".format(train_features_vec.shape))","metadata":{"id":"Smmmf92vHMjz","outputId":"3ad2a0f8-c954-47d7-ad28-f9bb429802af","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, (inputs_batch, labels_batch) in enumerate(train_generator):\n    if i % 50 == 0:\n        print(i)\n    if i * batch_size >= nTrain:\n        break   \n    train_labels[i * batch_size : (i + 1) * batch_size] = labels_batch","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:13:46.032133Z","iopub.execute_input":"2021-05-21T08:13:46.032469Z","iopub.status.idle":"2021-05-21T08:17:36.021805Z","shell.execute_reply.started":"2021-05-21T08:13:46.032422Z","shell.execute_reply":"2021-05-21T08:17:36.020798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('train_features_vec', train_features_vec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nVal = 6000\nvalidation_features = np.zeros(shape=(nVal, 7, 7, 512))\nvalidation_labels = np.zeros(shape=(nVal,4))","metadata":{"id":"WJP17Xb5Ho5E","execution":{"iopub.status.busy":"2021-05-21T08:17:36.023338Z","iopub.execute_input":"2021-05-21T08:17:36.023722Z","iopub.status.idle":"2021-05-21T08:17:36.028376Z","shell.execute_reply.started":"2021-05-21T08:17:36.023685Z","shell.execute_reply":"2021-05-21T08:17:36.027288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate through the batches of validation images and labels\nfor i, (inputs_batch, labels_batch) in enumerate(validation_generator):\n    if i % 50 == 0:\n        print(i)\n    if i * batch_size >= nVal:\n        break\n    features_batch = vgg_conv.predict(inputs_batch)\n    validation_features[i * batch_size : (i + 1) * batch_size] = features_batch\n    validation_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n\n# reshape validation_features into vector \nvalidation_features_vec = np.reshape(validation_features, (nVal, 7 * 7 * 512))\nprint(\"Validation features: {}\".format(validation_features_vec.shape))","metadata":{"id":"1thx6kD2HuX1","outputId":"03c3fbc2-b13b-4ddd-9080-f4fd43f66308","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, (inputs_batch, labels_batch) in enumerate(validation_generator):\n    if i % 50 == 0:\n        print(i)\n    if i * batch_size >= nVal:\n        break\n    validation_labels[i * batch_size : (i + 1) * batch_size] = labels_batch","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:17:36.030239Z","iopub.execute_input":"2021-05-21T08:17:36.030792Z","iopub.status.idle":"2021-05-21T08:18:38.324041Z","shell.execute_reply.started":"2021-05-21T08:17:36.030753Z","shell.execute_reply":"2021-05-21T08:18:38.323223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('validation_features_vec', validation_features_vec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features_vec = np.load('../input/features/train_features_vec.npy')\nvalidation_features_vec = np.load('../input/features/validation_features_vec.npy')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:18:38.325299Z","iopub.execute_input":"2021-05-21T08:18:38.325642Z","iopub.status.idle":"2021-05-21T08:19:42.756539Z","shell.execute_reply.started":"2021-05-21T08:18:38.325606Z","shell.execute_reply":"2021-05-21T08:19:42.755518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras import Sequential, optimizers\n\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=7 * 7 * 512))\nmodel.add(Dense(128, activation='relu', input_dim=512))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(4, activation='softmax'))","metadata":{"id":"E31qnaN1u74k","execution":{"iopub.status.busy":"2021-05-21T11:38:18.364237Z","iopub.execute_input":"2021-05-21T11:38:18.364579Z","iopub.status.idle":"2021-05-21T11:38:18.3975Z","shell.execute_reply.started":"2021-05-21T11:38:18.36455Z","shell.execute_reply":"2021-05-21T11:38:18.396724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n# configure the model for training\nmodel.compile(optimizer=optimizers.Adam(),\n              loss='categorical_crossentropy',\n              metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n\n# use the train and validation feature vectors\nhistory = model.fit(train_features_vec,\n                    train_labels,\n                    epochs=20,\n                    batch_size=batch_size,\n                    validation_data=(validation_features_vec,\n                                     validation_labels))","metadata":{"id":"rab8WpmEA2TX","outputId":"ae800974-0f2b-48e3-cb81-c28d5aa86c1e","execution":{"iopub.status.busy":"2021-05-21T08:22:53.155972Z","iopub.execute_input":"2021-05-21T08:22:53.156302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use the train and validation feature vectors\nhistory = model.fit(train_features_vec,\n                    train_labels,\n                    epochs=20,\n                    batch_size=batch_size,\n                    validation_data=(validation_features_vec,\n                                     validation_labels))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T07:51:50.259144Z","iopub.execute_input":"2021-05-21T07:51:50.259506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the list of all validation file names\nfnames = validation_generator.filenames\n\n# get the list of the corresponding classes\nground_truth = validation_generator.classes[:300]\n\n# get the dictionary of classes\nlabel2index = validation_generator.class_indices\n\n# obtain the list of classes\nidx2label = list(label2index.keys())\nprint(\"The list of classes: \", idx2label)","metadata":{"id":"7fRECdyKJBI-","outputId":"f500fd23-ea0c-4341-8a05-7ab32dce25e8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict_classes(validation_features_vec)\nprob = model.predict(validation_features_vec)","metadata":{"id":"Vqcks7WNJDbL","outputId":"0b23ee0f-dc54-410c-8c33-49d061de0787"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"errors = np.where(predictions != ground_truth)[0]\nprint(\"Number of errors = {}/{}\".format(len(errors),nVal))","metadata":{"id":"Kx4pBUVyJFmw","outputId":"b96a87cf-cf9a-4596-fbff-6a7fa6f3e438"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\nimport matplotlib.pyplot as plt\nfor i in range(len(errors)):\n    pred_class = np.argmax(prob[errors[i]])\n    pred_label = idx2label[pred_class]\n    \n    print('Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n        fnames[errors[i]].split('/')[0],\n        pred_label,\n        prob[errors[i]][pred_class]))\n    \n    original = load_img('{}/{}'.format(photos_dir,fnames[errors[i]]))\n    plt.axis('off')\n    plt.imshow(original)\n    plt.show()","metadata":{"id":"AqrcetB8JQlf","outputId":"dda8ead9-e143-4781-bba5-beadb3d915c6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Inception fine-tuning","metadata":{"id":"ipH902bKqoj4"}},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n\n# create the base pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dense(1024, activation='relu')(x)\n# and a logistic layer --  we have 4 classes\npredictions = Dense(4, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)","metadata":{"id":"P-HJbY6gqoj4","outputId":"af68142e-251a-4b4b-a746-6f4105633495","execution":{"iopub.status.busy":"2021-05-21T08:30:28.808128Z","iopub.execute_input":"2021-05-21T08:30:28.808484Z","iopub.status.idle":"2021-05-21T08:30:35.113971Z","shell.execute_reply.started":"2021-05-21T08:30:28.808427Z","shell.execute_reply":"2021-05-21T08:30:35.113075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n              metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])","metadata":{"id":"vXdUcQx4qoj6","execution":{"iopub.status.busy":"2021-05-21T08:31:00.073272Z","iopub.execute_input":"2021-05-21T08:31:00.073658Z","iopub.status.idle":"2021-05-21T08:31:00.114529Z","shell.execute_reply.started":"2021-05-21T08:31:00.073621Z","shell.execute_reply":"2021-05-21T08:31:00.113662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"totalTrain=33600\ntotalVal=9600\nNUM_EPOCHS=10\n# train the model on the new data for a few epochs\nmodel.fit(train_generator,\n    steps_per_epoch=totalTrain // batch_size,\n    validation_data=valid_generator,\n    validation_steps=totalVal // batch_size,\n    epochs=NUM_EPOCHS)\n\n# at this point, the top layers are well trained and we can start fine-tuning\n# convolutional layers from inception V3. We will freeze the bottom N layers\n# and train the remaining top layers.","metadata":{"id":"i4m1c31Sqoj6","outputId":"6b4a3aad-e8cc-47e5-f93f-908e10210da1","execution":{"iopub.status.busy":"2021-05-21T08:31:46.117057Z","iopub.execute_input":"2021-05-21T08:31:46.117398Z","iopub.status.idle":"2021-05-21T09:19:03.426613Z","shell.execute_reply.started":"2021-05-21T08:31:46.117366Z","shell.execute_reply":"2021-05-21T09:19:03.425704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's visualize layer names and layer indices to see how many layers\n# we should freeze:\nfor i, layer in enumerate(base_model.layers):\n    print(i, layer.name)\n\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 249 layers and unfreeze the rest:\nfor layer in model.layers[:249]:\n    layer.trainable = False\nfor layer in model.layers[249:]:\n    layer.trainable = True","metadata":{"id":"JzJ319emqoj8","execution":{"iopub.status.busy":"2021-05-21T09:24:43.859596Z","iopub.execute_input":"2021-05-21T09:24:43.860032Z","iopub.status.idle":"2021-05-21T09:24:43.962868Z","shell.execute_reply.started":"2021-05-21T09:24:43.859992Z","shell.execute_reply":"2021-05-21T09:24:43.96056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"totalTrain = 33600\ntotalVal = 9600\n# we need to recompile the model for these modifications to take effect\n# we use SGD with a low learning rate\nfrom tensorflow.keras.optimizers import SGD\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy',\n    metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n\n# we train our model again (this time fine-tuning the top 2 inception blocks\n# alongside the top Dense layers\nmodel.fit(train_generator,\n    steps_per_epoch=totalTrain // batch_size,\n    validation_data=valid_generator,\n    validation_steps=totalVal // batch_size,\n    epochs=10)","metadata":{"id":"H7qAXwBSqoj9","execution":{"iopub.status.busy":"2021-05-21T09:25:57.809612Z","iopub.execute_input":"2021-05-21T09:25:57.810029Z","iopub.status.idle":"2021-05-21T10:05:50.78095Z","shell.execute_reply.started":"2021-05-21T09:25:57.809996Z","shell.execute_reply":"2021-05-21T10:05:50.78001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Resnet fine tuning","metadata":{"id":"Oq3xvui8qoj-"}},{"cell_type":"code","source":"totalTrain=4000\ntotalVal = 1000\n# initialize the initial learning rate, batch size, and number of\n# epochs to train for\nINIT_LR = 1e-4\nNUM_EPOCHS = 20\n# define the path to the serialized output model after training\nMODEL_PATH = \"label_detector.model\"","metadata":{"id":"Wu8EiifDqoj_","execution":{"iopub.status.busy":"2021-05-23T08:28:29.853832Z","iopub.execute_input":"2021-05-23T08:28:29.854195Z","iopub.status.idle":"2021-05-23T08:28:29.858596Z","shell.execute_reply.started":"2021-05-23T08:28:29.854161Z","shell.execute_reply":"2021-05-23T08:28:29.857427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try k-fold cross validation","metadata":{"id":"2tqMNp8IL4Nc"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nY = photos['label']\n\nkf = KFold(n_splits = 5)                         ","metadata":{"id":"CNf7wwAbAnUT","execution":{"iopub.status.busy":"2021-05-20T11:25:34.986426Z","iopub.execute_input":"2021-05-20T11:25:34.986754Z","iopub.status.idle":"2021-05-20T11:25:35.525014Z","shell.execute_reply.started":"2021-05-20T11:25:34.986725Z","shell.execute_reply":"2021-05-20T11:25:35.524207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idg = ImageDataGenerator(width_shift_range=0.1,\n                         height_shift_range=0.1,\n                         zoom_range=0.3,\n                         fill_mode='nearest',\n                         horizontal_flip = True,\n                         rescale=1./255)","metadata":{"id":"ar0TVZt9MO4r","execution":{"iopub.status.busy":"2021-05-20T11:25:38.57989Z","iopub.execute_input":"2021-05-20T11:25:38.580209Z","iopub.status.idle":"2021-05-20T11:25:38.585078Z","shell.execute_reply.started":"2021-05-20T11:25:38.580179Z","shell.execute_reply":"2021-05-20T11:25:38.58426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_name(k):\n    return 'model_'+str(k)+'.h5'","metadata":{"id":"w-QkTIvbMRDW","execution":{"iopub.status.busy":"2021-05-20T11:25:40.967821Z","iopub.execute_input":"2021-05-20T11:25:40.968159Z","iopub.status.idle":"2021-05-20T11:25:40.97238Z","shell.execute_reply.started":"2021-05-20T11:25:40.968127Z","shell.execute_reply":"2021-05-20T11:25:40.97125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n# create the base pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dense(1024, activation='relu')(x)\n# and a logistic layer --  we have 4 classes\npredictions = Dense(4, activation='softmax')(x)","metadata":{"id":"n4yhUCcwNvk9","outputId":"7e924c05-30f2-4cc5-d84a-231680ab9755","execution":{"iopub.status.busy":"2021-05-20T11:25:44.953035Z","iopub.execute_input":"2021-05-20T11:25:44.953415Z","iopub.status.idle":"2021-05-20T11:25:51.091485Z","shell.execute_reply.started":"2021-05-20T11:25:44.953381Z","shell.execute_reply":"2021-05-20T11:25:51.090479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VALIDATION_ACCURACY = []\nVALIDAITON_LOSS = []\nbatch_size = 50\nnum_epochs=10\nphotos_dir = '../input/photos/photos'\nnTrain = 48000\nsave_dir = '/saved_models/'\nfold_var = 1\n\nfor train_index, val_index in kf.split(np.zeros(len(photos)),Y):\n    training_data = photos.iloc[train_index]\n    validation_data = photos.iloc[val_index]\n    \n    train_data_generator = idg.flow_from_dataframe(photos[:40000], photos_dir, x_col = 'photo_path', y_col = 'label',\n                                                  target_size=(224, 224),\n                                                  batch_size=batch_size,\n                                                  class_mode='categorical', shuffle=True)\n\n    valid_data_generator  = idg.flow_from_dataframe(photos[40000:46000],\n                                                  photos_dir, x_col = 'photo_path', y_col = 'label',\n                                                  target_size=(224, 224),\n                                                  batch_size=batch_size,color_mode=\"rgb\",\n                                                  class_mode='categorical', shuffle=True)\n    \n    # CREATE NEW MODEL\n    # this is the model we will train\n    model = Model(inputs=base_model.input, outputs=predictions)\n    # COMPILE NEW MODEL\n    for layer in base_model.layers:\n      layer.trainable = False\n\n    # compile the model (should be done *after* setting layers to non-trainable)\n    model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n                  metrics=[\"accuracy\"])\n    \n    # CREATE CALLBACKS\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(fold_var), \n                                                  monitor='val_accuracy', verbose=1, \n                                                  save_best_only=True, mode='max')\n    callbacks_list = [checkpoint]\n    # There can be other callbacks, but just showing one because it involves the model name\n    # This saves the best model\n    # FIT THE MODEL\n    history = model.fit(train_data_generator,\n                      epochs=num_epochs,\n                      callbacks=callbacks_list,\n                      validation_data=valid_data_generator)\n    #PLOT HISTORY\n    #\t\t:\n    #\t\t:\n    \n    # LOAD BEST MODEL to evaluate the performance of the model\n    model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n    \n    results = model.evaluate(valid_data_generator)\n    results = dict(zip(model.metrics_names,results))\n    \n    VALIDATION_ACCURACY.append(results['accuracy'])\n    VALIDAITON_LOSS.append(results['loss'])\n    \n    tf.keras.backend.clear_session()\n    \n    fold_var += 1\n    ","metadata":{"id":"szqxdlzaMTbd","execution":{"iopub.status.busy":"2021-05-20T11:29:03.251161Z","iopub.execute_input":"2021-05-20T11:29:03.251524Z","iopub.status.idle":"2021-05-20T13:44:27.386715Z","shell.execute_reply.started":"2021-05-20T11:29:03.251494Z","shell.execute_reply":"2021-05-20T13:44:27.38343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize the training training data augmentation object\ntrainAug = ImageDataGenerator(\n\trotation_range=25,\n\tzoom_range=0.1,\n\twidth_shift_range=0.1,\n\theight_shift_range=0.1,\n\tshear_range=0.2,\n\thorizontal_flip=True,\n\tfill_mode=\"nearest\")\n# initialize the validation/testing data augmentation object (which\n# we'll be adding mean subtraction to)\nvalAug = ImageDataGenerator()\n# define the ImageNet mean subtraction (in RGB order) and set the\n# the mean subtraction value for each of the data augmentation\n# objects\nmean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\ntrainAug.mean = mean\nvalAug.mean = mean","metadata":{"id":"TUsmYZO0qoj_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the ResNet-50 network, ensuring the head FC layer sets are left\n# off\nprint(\"[INFO] preparing model...\")\nbaseModel = ResNet50(weights=\"imagenet\", include_top=False,\n    input_tensor=Input(shape=(224, 224, 3)))\n# construct the head of the model that will be placed on top of the\n# the base model\nheadModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(7, 7))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(256, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(4, activation=\"softmax\")(headModel)\n# place the head FC model on top of the base model (this will become\n# the actual model we will train)\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n# loop over all layers in the base model and freeze them so they will\n# *not* be updated during the training process\nfor layer in baseModel.layers:\n    layer.trainable = False","metadata":{"id":"rtNml0UpqokA","outputId":"00f57c3e-6e70-4a71-dc71-5e4fb45b7755","execution":{"iopub.status.busy":"2021-05-23T08:37:28.717873Z","iopub.execute_input":"2021-05-23T08:37:28.718195Z","iopub.status.idle":"2021-05-23T08:37:30.093827Z","shell.execute_reply.started":"2021-05-23T08:37:28.718165Z","shell.execute_reply":"2021-05-23T08:37:30.09278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile the model\nopt = Adam(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n    metrics=[\"accuracy\"])\n# train the model\nprint(\"[INFO] training model...\")\ntotalTrain=38400\ntotalVal = 9600\nH = model.fit_generator(\n    train_generator,\n    steps_per_epoch=totalTrain // batch_size,\n    validation_data=validation_generator,\n    validation_steps=totalVal // batch_size,\n    epochs=NUM_EPOCHS)","metadata":{"id":"sisxAxFPqokB","outputId":"cf7429e2-c31d-4762-b000-0e058b009f5e","execution":{"iopub.status.busy":"2021-05-23T08:37:33.929723Z","iopub.execute_input":"2021-05-23T08:37:33.930135Z","iopub.status.idle":"2021-05-23T10:05:29.15812Z","shell.execute_reply.started":"2021-05-23T08:37:33.930089Z","shell.execute_reply":"2021-05-23T10:05:29.15725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H = model.fit_generator(\n    train_generator,\n    steps_per_epoch=totalTrain // batch_size,\n    validation_data=validation_generator,\n    validation_steps=totalVal // batch_size,\n    epochs=NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T08:30:53.232629Z","iopub.execute_input":"2021-05-23T08:30:53.232962Z","iopub.status.idle":"2021-05-23T08:37:05.450929Z","shell.execute_reply.started":"2021-05-23T08:30:53.232934Z","shell.execute_reply":"2021-05-23T08:37:05.448051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testGen = datagen.flow_from_dataframe(small_photos[6000:],\n    photos_dir, x_col = 'photo_path', y_col = 'label',\n    target_size=(224, 224),\n    batch_size=batch_size,color_mode=\"rgb\",\n    class_mode='categorical',\n    shuffle=True)","metadata":{"id":"9CiY-XlkqokB","outputId":"8c4910db-fb8a-4426-9a54-69ee39cba74e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('resnet_labels.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\nmodel = load_model('../input/trained-model-resnet/resnet_labels.h5')","metadata":{"execution":{"iopub.status.busy":"2021-05-23T08:29:18.621997Z","iopub.execute_input":"2021-05-23T08:29:18.622357Z","iopub.status.idle":"2021-05-23T08:29:22.78883Z","shell.execute_reply.started":"2021-05-23T08:29:18.622303Z","shell.execute_reply":"2021-05-23T08:29:22.787841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INIT_LR = 1e-4\nNUM_EPOCHS = 1\nopt = Adam(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n    metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T18:22:10.448695Z","iopub.execute_input":"2021-05-20T18:22:10.449034Z","iopub.status.idle":"2021-05-20T18:22:10.482867Z","shell.execute_reply.started":"2021-05-20T18:22:10.449009Z","shell.execute_reply":"2021-05-20T18:22:10.481059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"totalTrain = 33600\ntotalVal = 9600\nmodel.fit(train_generator,\n    steps_per_epoch=totalTrain // batch_size,\n    validation_data=valid_generator,\n    validation_steps=totalVal // batch_size,\n    epochs=20)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T18:22:13.742497Z","iopub.execute_input":"2021-05-20T18:22:13.742812Z","iopub.status.idle":"2021-05-20T19:17:09.93316Z","shell.execute_reply.started":"2021-05-20T18:22:13.742785Z","shell.execute_reply":"2021-05-20T19:17:09.932343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ntotalTest=6600\n# reset the testing generator and then use our trained model to\n# make predictions on the data\nprint(\"[INFO] evaluating network...\")\ntest_generator.reset()\npredIdxs = model.predict_generator(test_generator,\n\tsteps=(totalTest // batch_size) + 1)\n# for each image in the testing set we need to find the index of the\n# label with corresponding largest predicted probability\npredIdxs = np.argmax(predIdxs, axis=1)\n# show a nicely formatted classification report\nprint(classification_report(test_generator.classes, predIdxs,\n\ttarget_names=test_generator.class_indices.keys()))\n# serialize the model to disk\n#print(\"[INFO] saving model...\")\n#MODEL_PATH = \"inceptionv3.model\"\n#model.save(MODEL_PATH, save_format=\"h5\")","metadata":{"id":"EbqQKzO7qokC","outputId":"765230bc-fbe2-4c21-aa42-18dddc28a2fa","execution":{"iopub.status.busy":"2021-05-21T10:08:10.766776Z","iopub.execute_input":"2021-05-21T10:08:10.767183Z","iopub.status.idle":"2021-05-21T10:09:01.277949Z","shell.execute_reply.started":"2021-05-21T10:08:10.767138Z","shell.execute_reply":"2021-05-21T10:09:01.27696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the training loss and accuracy\nN = NUM_EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on Dataset\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.show()","metadata":{"id":"mgNsa7bAqokE","outputId":"e7e6e41a-8545-4b98-f085-3a206667f736","execution":{"iopub.status.busy":"2021-05-23T10:05:43.840002Z","iopub.execute_input":"2021-05-23T10:05:43.840343Z","iopub.status.idle":"2021-05-23T10:05:44.024897Z","shell.execute_reply.started":"2021-05-23T10:05:43.840293Z","shell.execute_reply":"2021-05-23T10:05:44.023769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Check predictions on my gallery","metadata":{"id":"Ra1by6iEshva"}},{"cell_type":"code","source":"feature_test_path = '/content/drive/MyDrive/для диплома/testgallery'\nfeature_test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=feature_test_path,\n                                                                  class_mode=None,\n                                                                  batch_size=batch_size,\n                                                                  target_size=(224, 224),\n                                                                  color_mode=\"rgb\",\n                                                                  shuffle=False)","metadata":{"id":"2eA2xQpwLMMI","outputId":"228b0024-075d-4263-9b97-c355f3e50502"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport cv2\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom keras.applications.resnet50 import preprocess_input\nfor filename in glob.iglob('../input/testgallery//*.jpg'):\n    img = image.load_img(filename, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x= preprocess_input(x)\n    #feature_vector = getFeatureVector(vgg_conv, filename)\n    preds = model.predict(x)\n    pred_class = np.argmax(preds)\n    pred_label = idx2label[pred_class]\n    print('predicted label is ', pred_label)\n    plt.imshow(img)\n    plt.show()","metadata":{"id":"hawKSJ7zqokI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing import image\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\nimg = image.load_img('D:/Downloads/Downloads/diploma/testgallery//IMG_20191027_163506.jpg', target_size=(299, 299))\n\nx = image.img_to_array(img)\n\nx = np.expand_dims(x, axis=0)\n\nx = preprocess_input(x)\n\npreds = model.predict(x)# расшифровать результаты \n#в список кортежей (класс, описание, вероятность)# (по одному такому списку для каждой выборки в партии).","metadata":{"id":"X0hGWPlgqokJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the dictionary of classes\nlabel2index = validation_generator.class_indices\n\n# obtain the list of classes\nidx2label = list(label2index.keys())\nprint(\"The list of classes: \", idx2label)","metadata":{"id":"z39hia64qokK","outputId":"ef20b181-d39e-45b6-9e59-8db96b414fcc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_class = np.argmax(preds)\npred_label = idx2label[pred_class]\nprint(pred_class, pred_label)\nplt.imshow(img)","metadata":{"id":"ad9yS_G5qokL","outputId":"db589f73-f0c9-492a-fcf9-e13014138e08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\nimport matplotlib.pyplot as plt\nfor i in range(len(36)):\n    pred_class = np.argmax(prob[i])\n    pred_label = idx2label[pred_class]\n    \n    print('Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n        photos['label'][photos['photo_path']==fnames[errors[i]].split('/')[0]],\n        pred_label,\n        prob[errors[i]][pred_class]))\n    \n    original = load_img('{}/{}'.format(train_dir,fnames[errors[i]]))\n    plt.axis('off')\n    plt.imshow(original)\n    plt.show()","metadata":{"id":"LvkQJx7UqokL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ambience classification","metadata":{}},{"cell_type":"markdown","source":"### Dataset only inside","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ambiences = ['touristy', 'hipster', 'romantic', 'intimate', 'trendy', 'upscale', 'classy', 'casual']","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:42.458171Z","iopub.execute_input":"2021-05-22T16:50:42.458505Z","iopub.status.idle":"2021-05-22T16:50:42.462797Z","shell.execute_reply.started":"2021-05-22T16:50:42.458475Z","shell.execute_reply":"2021-05-22T16:50:42.461661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\nfor colname in ambiences:\n    df[colname]=0\n    print('working on '+colname)\n    for row in df.itertuples():\n      if row.attributes != None:\n          d = dict(row.attributes)\n          try:\n            #print(ast.literal_eval(d['Ambience']))\n            if ast.literal_eval(d['Ambience'])!= None:\n                try:\n                  if ast.literal_eval(d['Ambience'])[colname] == None:\n                    df[colname][row.Index] = 0\n                  else:  \n                    df[colname][row.Index] = int(ast.literal_eval(d['Ambience'])[colname])\n                except KeyError:\n                    continue\n          except KeyError:\n                continue ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-22T16:50:45.448109Z","iopub.execute_input":"2021-05-22T16:50:45.448454Z","iopub.status.idle":"2021-05-22T16:51:44.092615Z","shell.execute_reply.started":"2021-05-22T16:50:45.448424Z","shell.execute_reply":"2021-05-22T16:51:44.091719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df['label'] == 'inside']\ndf","metadata":{"execution":{"iopub.status.busy":"2021-05-21T12:22:08.229597Z","iopub.execute_input":"2021-05-21T12:22:08.229922Z","iopub.status.idle":"2021-05-21T12:22:08.315207Z","shell.execute_reply.started":"2021-05-21T12:22:08.229894Z","shell.execute_reply":"2021-05-21T12:22:08.314453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen=ImageDataGenerator(rescale=1./255.)\ntest_datagen=ImageDataGenerator(rescale=1./255.)\n\ncolumns=ambiences\nnTrain=48000 \nphotos_dir = '../input/photos/photos'\n\n\ntrain_generator=datagen.flow_from_dataframe(dataframe=df[:int(nTrain*0.7)],\n                                            directory=photos_dir,\n                                            x_col='photo_path',\n                                            y_col=columns,\n                                            batch_size=50,\n                                            seed=42,\n                                            shuffle=True,\n                                            class_mode=\"raw\",\n                                            target_size=(224,224))\nvalid_generator=test_datagen.flow_from_dataframe(dataframe=df[int(nTrain*0.7):int(nTrain*0.9)],\n                                                 directory=photos_dir,\n                                                 x_col='photo_path',y_col=columns,\n                                                 batch_size=50,seed=42,shuffle=True,\n                                                 class_mode=\"raw\",\n                                                 target_size=(224,224))\n'''test_generator=test_datagen.flow_from_dataframe(dataframe=df[int(nTrain*0.9):],\n                                                directory=photos_dir,\n                                                x_col='photo_path',\n                                                batch_size=1,\n                                                seed=42,\n                                                shuffle=False,\n                                                class_mode=None,\n                                                target_size=(256,256))'''","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:52:42.994236Z","iopub.execute_input":"2021-05-22T16:52:42.994574Z","iopub.status.idle":"2021-05-22T16:54:12.918987Z","shell.execute_reply.started":"2021-05-22T16:52:42.994545Z","shell.execute_reply":"2021-05-22T16:54:12.918193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inception for ambience","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n\n# create the base pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dense(1024, activation='relu')(x)\n# and a logistic layer --  we have 4 classes\npredictions = Dense(8, activation='sigmoid')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:54:44.239115Z","iopub.execute_input":"2021-05-22T16:54:44.239445Z","iopub.status.idle":"2021-05-22T16:54:49.429524Z","shell.execute_reply.started":"2021-05-22T16:54:44.239416Z","shell.execute_reply":"2021-05-22T16:54:49.428685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"totalTrain = 33600\ntotalVal = 9600\nbatch_size = 50\nSTEP_SIZE_TRAIN = totalTrain // batch_size\nSTEP_SIZE_VALID = totalVal // batch_size","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:55:06.772576Z","iopub.execute_input":"2021-05-22T16:55:06.772922Z","iopub.status.idle":"2021-05-22T16:55:06.777196Z","shell.execute_reply.started":"2021-05-22T16:55:06.772871Z","shell.execute_reply":"2021-05-22T16:55:06.77612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy',\n    metrics=[tf.keras.metrics.Precision(),'accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:55:20.883716Z","iopub.execute_input":"2021-05-22T16:55:20.884171Z","iopub.status.idle":"2021-05-22T16:55:20.917355Z","shell.execute_reply.started":"2021-05-22T16:55:20.884133Z","shell.execute_reply":"2021-05-22T16:55:20.916582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_generator,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=valid_generator,\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=10\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T17:26:47.578287Z","iopub.execute_input":"2021-05-22T17:26:47.578618Z","iopub.status.idle":"2021-05-22T18:04:02.608497Z","shell.execute_reply.started":"2021-05-22T17:26:47.578587Z","shell.execute_reply":"2021-05-22T18:04:02.607655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 249 layers and unfreeze the rest:\nfor layer in model.layers[:249]:\n    layer.trainable = False\nfor layer in model.layers[249:]:\n    layer.trainable = True","metadata":{"execution":{"iopub.status.busy":"2021-05-21T13:02:57.084037Z","iopub.execute_input":"2021-05-21T13:02:57.084388Z","iopub.status.idle":"2021-05-21T13:02:57.101174Z","shell.execute_reply.started":"2021-05-21T13:02:57.084355Z","shell.execute_reply":"2021-05-21T13:02:57.100301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need to recompile the model for these modifications to take effect\n# we use SGD with a low learning rate\nfrom tensorflow.keras.optimizers import SGD\nimport tensorflow as tf\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy',\n    metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n\n# we train our model again (this time fine-tuning the top 2 inception blocks\n# alongside the top Dense layers\nmodel.fit(train_generator,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=valid_generator,\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=7\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T13:03:00.623719Z","iopub.execute_input":"2021-05-21T13:03:00.624043Z","iopub.status.idle":"2021-05-21T13:22:27.332338Z","shell.execute_reply.started":"2021-05-21T13:03:00.624012Z","shell.execute_reply":"2021-05-21T13:22:27.331535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## resnet for ambience","metadata":{}},{"cell_type":"code","source":"\nbaseModel = ResNet50(weights=\"imagenet\", include_top=False,\n                     input_tensor=Input(shape=(224, 224, 3)))\n# construct the head of the model that will be placed on top of the\n# the base model\nheadModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(7, 7))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(512, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(8, activation=\"sigmoid\")(headModel)\n# place the head FC model on top of the base model (this will become\n# the actual model we will train)\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n# loop over all layers in the base model and freeze them so they will\n# *not* be updated during the training process\nfor layer in baseModel.layers:\n    layer.trainable = False\nmodel.compile(optimizers.RMSprop(lr=0.0001, decay=1e-6),loss=\"binary_crossentropy\",metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T13:35:01.160332Z","iopub.execute_input":"2021-05-21T13:35:01.160685Z","iopub.status.idle":"2021-05-21T13:35:02.659508Z","shell.execute_reply.started":"2021-05-21T13:35:01.160656Z","shell.execute_reply":"2021-05-21T13:35:02.658619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\nSTEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\nSTEP_SIZE_TEST=test_generator.n//test_generator.batch_size\nmodel.fit_generator(generator=train_generator,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=valid_generator,\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=7\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T13:35:06.148563Z","iopub.execute_input":"2021-05-21T13:35:06.148886Z","iopub.status.idle":"2021-05-21T14:09:36.753591Z","shell.execute_reply.started":"2021-05-21T13:35:06.148856Z","shell.execute_reply":"2021-05-21T14:09:36.7527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature extraction for ambience","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import vgg16\n\nvgg_conv = vgg16.VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(224, 224, 3))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T17:02:57.357784Z","iopub.execute_input":"2021-05-21T17:02:57.358152Z","iopub.status.idle":"2021-05-21T17:03:00.304108Z","shell.execute_reply.started":"2021-05-21T17:02:57.358115Z","shell.execute_reply":"2021-05-21T17:03:00.303058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nTrain = 14000\nbatch_size = 50\n# the defined shape is equal to the network output tensor shape\ntrain_features = np.zeros(shape=(nTrain, 7, 7, 512))\ntrain_labels = np.zeros(shape=(nTrain,8))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T17:03:07.535278Z","iopub.execute_input":"2021-05-21T17:03:07.537877Z","iopub.status.idle":"2021-05-21T17:03:07.545122Z","shell.execute_reply.started":"2021-05-21T17:03:07.53783Z","shell.execute_reply":"2021-05-21T17:03:07.543855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate through the batches of train images and labels\nfor i, (inputs_batch, labels_batch) in enumerate(train_generator):\n    if i % 50 == 0:\n        print(i)\n    if i * batch_size >= nTrain:\n        break   \n    # pass the images through the network\n    features_batch = vgg_conv.predict(inputs_batch)\n    train_features[i * batch_size : (i + 1) * batch_size] = features_batch\n    train_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n# reshape train_features into vector       \ntrain_features_vec = np.reshape(train_features, (nTrain, 7 * 7 * 512))\nprint(\"Train features: {}\".format(train_features_vec.shape))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T17:03:12.147804Z","iopub.execute_input":"2021-05-21T17:03:12.148212Z","iopub.status.idle":"2021-05-21T17:31:30.269931Z","shell.execute_reply.started":"2021-05-21T17:03:12.148175Z","shell.execute_reply":"2021-05-21T17:31:30.26866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('train_features', train_features)\nnp.save('train_labels', train_labels)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T17:34:08.350999Z","iopub.execute_input":"2021-05-21T17:34:08.351431Z","iopub.status.idle":"2021-05-21T17:34:14.390316Z","shell.execute_reply.started":"2021-05-21T17:34:08.351365Z","shell.execute_reply":"2021-05-21T17:34:14.389253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nVal = 4000\nvalidation_features = np.zeros(shape=(nVal, 7, 7, 512))\nvalidation_labels = np.zeros(shape=(nVal,8))\n# iterate through the batches of validation images and labels\nfor i, (inputs_batch, labels_batch) in enumerate(valid_generator):\n    if i % 50 == 0:\n        print(i)\n    if i * batch_size >= nVal:\n        break\n    features_batch = vgg_conv.predict(inputs_batch)\n    validation_features[i * batch_size : (i + 1) * batch_size] = features_batch\n    validation_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n\n# reshape validation_features into vector \nvalidation_features_vec = np.reshape(validation_features, (nVal, 7 * 7 * 512))\nprint(\"Validation features: {}\".format(validation_features_vec.shape))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T17:34:20.599119Z","iopub.execute_input":"2021-05-21T17:34:20.599586Z","iopub.status.idle":"2021-05-21T17:42:24.208442Z","shell.execute_reply.started":"2021-05-21T17:34:20.59955Z","shell.execute_reply":"2021-05-21T17:42:24.207195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras import Sequential, optimizers\n\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=7 * 7 * 512))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(8, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T17:43:43.745091Z","iopub.execute_input":"2021-05-21T17:43:43.745523Z","iopub.status.idle":"2021-05-21T17:43:43.781484Z","shell.execute_reply.started":"2021-05-21T17:43:43.745482Z","shell.execute_reply":"2021-05-21T17:43:43.780482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n# configure the model for training\nmodel.compile(optimizer=optimizers.RMSprop(lr=2e-4),\n              loss='binary_crossentropy',\n              metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n\n# use the train and validation feature vectors\nhistory = model.fit(train_features_vec,\n                    train_labels,\n                    epochs=10,\n                    batch_size=batch_size,\n                    validation_data=(validation_features_vec,\n                                     validation_labels))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T17:43:56.655613Z","iopub.execute_input":"2021-05-21T17:43:56.656004Z","iopub.status.idle":"2021-05-21T17:44:28.510844Z","shell.execute_reply.started":"2021-05-21T17:43:56.655965Z","shell.execute_reply":"2021-05-21T17:44:28.50989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use the train and validation feature vectors\nhistory = model.fit(train_features_vec,\n                    train_labels,\n                    epochs=20,\n                    batch_size=batch_size,\n                    validation_data=(validation_features_vec,\n                                     validation_labels))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T17:44:53.062586Z","iopub.execute_input":"2021-05-21T17:44:53.062954Z","iopub.status.idle":"2021-05-21T17:45:51.84957Z","shell.execute_reply.started":"2021-05-21T17:44:53.062921Z","shell.execute_reply":"2021-05-21T17:45:51.848648Z"},"trusted":true},"execution_count":null,"outputs":[]}]}